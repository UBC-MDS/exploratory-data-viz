---
params:
  dynamictitle: "module6_01"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module6/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module6/"
```

type: slides

# One-hot encoding

Notes: <br>

---

## From before ...

```{python include=FALSE}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
 
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
 
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]
 
X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
 
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train)
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)


X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]

pipe = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("reg", KNeighborsRegressor()),
    ]
)

X_toy = pd.DataFrame({'language':['English', 'Vietnamese', 'English', 'Mandarin', 
                                  'English', 'English', 'Mandarin', 'English', 
                                  'Vietnamese', 'Mandarin', 'French','Spanish', 
                                  'Mandarin', 'Hindi']})
                                  
X_toy

from sklearn.preprocessing import OrdinalEncoder

oe = OrdinalEncoder(dtype=int)
oe.fit(X_toy);
X_toy_ord = oe.transform(X_toy)

X_toy_ord


encoding_view = X_toy.assign(language_enc=X_toy_ord)

```

```{python}
encoding_view
```


Notes: 

In the last section, we saw that we can transform our categorical data into numeric data using `OrdinalEncoder`. Seems pretty standard and easy enough but we asked you a question in the last slide deck if we should always use this method. 

The answer is no. Can you see why? 

---

## What wrong with this?

```{python}
oe.categories_
```

```{python}
encoding_view.drop_duplicates()
```


Notes: 

What's the problem with this approach? 

If you look at the original values and compare them to the new transformed ones what do you notice?

We have imposed ordinality on the categorical data.

For example, imagine when you are calculating distances. Is it fair to say that French and Hindi are closer to one another than French and Spanish? 

In general, label encoding is useful if there is ordinality in your data and capturing it is important for your problem, e.g., `[cold, warm, hot]`. 

---

## One-hot encoding (OHE)


Ordinal encoding: 

```{python}
encoding_view[['language_enc']].head()
```


```{python include=FALSE}
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(handle_unknown='ignore', sparse=False)
enc.fit(X_toy)
X_toy_ohe = enc.transform(X_toy)
one_hot_df = pd.DataFrame(
    data=X_toy_ohe,
    columns=enc.get_feature_names(['language']),
    index=X_toy.index,
)
```

One-hot encoding: 

```{python}
one_hot_df.head()
```

Notes: 

So what do we do when our values are not truly ordinal categories? 

We can do something called **One hot encoding**!

Rather than assign integer labels to our data, we use it to create new binary columns to represent our categories.

Before we would transform one original column into one transformed column but in this case, we will transform one column into several transformed columns, one per category.

One-hot encoding creates new binary columns to represent our categories.

If we have ùëê categories in our column, we create ùëê new binary columns to represent those categories.
- Example: Imagine a language column which has the information on whether you 

- We can use sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank">`OneHotEncoder`</a>


---

## How to one-hot encode

```{python}
X_toy
```
Notes:

Let's take our `X_toy` and one-hot encode it. 

---


```{python}
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse=False, dtype='int')
ohe.fit(X_toy);
X_toy_ohe = ohe.transform(X_toy)

X_toy_ohe
```



Notes: 

We import the `OneHotEncoder` transformer from `sklearn` and then build our transformer. 

We fit and transform the data and exactly as before, our output from the `transform` function is a NumPy array.

---


```{python}
pd.DataFrame(
    data=X_toy_ohe,
    columns=enc.get_feature_names(['language']),
    index=X_toy.index,
)
```

Notes: 

We can convert it to a Pandas dataframe and see that instead of 1 column, we have 6!

---


```{python}
X_train.head()
```

```{python}
X_train['ocean_proximity'].unique()
```


Notes: 

Ok, so what should we use on our California housing data? 

`ocean_proximity` seems like an ordinal feature, however, looking at the possible categories seems a little less clear. 

How would you order these? 

Should `NEAR OCEAN` be higher in value than `NEAR BAY`? 

In unsure times, maybe one-hot encoding is the better option. 


---

## One hot encoding the California housing data

```{python}
ohe = OneHotEncoder(sparse=False, dtype="int")
ohe.fit(X_train[["ocean_proximity"]])
X_imp_ohe_train = ohe.transform(X_train[["ocean_proximity"]])

X_imp_ohe_train
```



Notes: 

Ok great we've transformed our data, however, Just like before, the transformer outputs a NumPy array. 


-----

```{python}
transformed_ohe = pd.DataFrame(
    data=X_imp_ohe_train,
    columns=ohe.get_feature_names(['ocean_proximity']),
    index=X_train.index,
)

transformed_ohe.head()
```

Notes: 


We can transform it into a dataframe to see the values more clearly. 


### But ....Now What?

How do we put this together with other columns in the data before fitting the model? 

We want to apply different transformations to different columns.  

We will explain that in the next section. 


---

# Let‚Äôs apply what we learned!

Notes: <br>