---
params:
  dynamictitle: "module6_09"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module6/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module6/"


housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
 
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
 
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]
 
X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
 
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train)
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)


X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]

```


type: slides

# *ColumnTransformer*

Notes: <br>

---

## Problem: We have different transformations for different columns

Before we fit our model, we want to apply different transformations on different columns.

- Numeric columns:
    - imputation 
    - scaling  
    
- Categorical columns:   
    - imputation      
    - one-hot encoding    
    

Notes: 

We can't use a pipeline since not all the transformations are occurring on every feature. 

We could do so without but then we would be violating the Golden Rule of Machine learning when we did cross-validation.

So we need a new tool and it's called `ColumnTransformer`!

---

## *ColumnTransformer*

<br>
<br>

<center><img src="/module6/column-transformer.png"  width = "90%" alt="404 image" /></center>
 <a href="https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37" target="_blank">Adapted from here. </a>

Notes: 


sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html" target="_blank">`ColumnTransformer`</a> makes this more manageable.

A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data. 

Otherwise, we might, for example, do the OHE on both train and test but forget to scale the test data.  

---

```{python}
from sklearn.compose import ColumnTransformer
```

```{python}
X_train.head()
```

Notes: 

We import `ColumnTransformer` from the `sklearn` library. 

And we will have to look at our data.

---

```{python}
X_train.dtypes
```
```{python}
numeric_features = [ "longitude",
                     "latitude",
                     "housing_median_age",
                     "households",
                     "median_income",
                     "rooms_per_household",
                     "bedrooms_per_household",
                     "population_per_household"]
                     
categorical_features = ["ocean_proximity"]
```


Notes: 

We must first identify the categorical and numeric columns.

---


```{python}
numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), 
           ("scaler", StandardScaler())]
)

categorical_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
           ("onehot", OneHotEncoder(handle_unknown="ignore"))]
)
```


Notes: 

Next, we build a pipeline for our dataset.

This means we need to make at least 2 preprocessing pipelines; one for the categorical and one for the numeric features! 

(If we needed to use the ordinal encoder for binary data or ordinal features then we would need a third.)


---

```{python}
col_transformer = ColumnTransformer(
    transformers=[
        ("numeric", numeric_transformer, numeric_features),
        ("categotical", categorical_transformer, categorical_features)
    ], 
    remainder='passthrough'    
)
```

Notes: 

We then call the numeric and categorical features with their respective transformers in `ColumnTransformer()`.

The `ColumnTransformer` syntax is somewhat similar to that of `Pipeline` in that you pass in a list of tuples.

But This time, each tuple has 3 values instead of 2: (name of the step, transformer object, list of columns)

A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.

Otherwise, we might, for example, do the OHE on both train and test but forget to scale the test data.

`remainder="passthrough"`:   

- The `ColumnTransformer` will automatically remove columns that are not being transformed.      
- We can use `remainder="passthrough"` of `ColumnTransformer` to keep the other columns intact.      

---

```{python}
col_transformer.fit(X_train)
```

```{python}
X_train_pp = col_transformer.transform(X_train)
```

Notes: 

When we `fit` with the `col_transformer`, it calls `fit` on ***all*** the transformers.

And when we transform with the preprocessor, it calls `transform` on ***all*** the transformers.

---

```{python}
onehot_cols = col_transformer.named_transformers_["categotical"].named_steps["onehot"].get_feature_names(categorical_features)
onehot_cols
```
```{python}
columns = numeric_features + list(onehot_cols)
columns
```


Notes: 

We can get the new names of the columns that were generated by the one-hot encoding.

Combining this with the numeric feature names gives us all the column names.

---

```{python}
main_pipe = Pipeline(
    steps=[
        ("preprocessor", col_transformer), # <-- this is the ColumnTransformer!
        ("reg", KNeighborsRegressor())])
```

```{python}
with_categorical_scores = cross_validate(main_pipe, X_train, y_train, return_train_score=True)
pd.DataFrame(with_categorical_scores)
```

Notes: 

Now we use a main pipeline to transform all the data and build a model. 

Scaling and one hot encoding are now applied at the same time!

We can then use `cross_validate()` and find our mean training and validation scores!

---

```{python}
pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("reg", KNeighborsRegressor())])
        
pipe.fit(X_train.drop(columns=['ocean_proximity']), y_train);
```

```{python}
no_categorical_scores = cross_validate(pipe, X_train.drop(columns=['ocean_proximity']), y_train, return_train_score=True)
pd.DataFrame(no_categorical_scores)
```



Notes: 

Let's compare what we did before without ColumTransformer and without categorical columns and observe how adding the `ocean_proximity` column changes our results. 

---

```{python}
pd.DataFrame(no_categorical_scores).mean()
```


```{python}
pd.DataFrame(with_categorical_scores).mean()
```


Notes: 

We can see here that adding and one hot encoding our `ocean_proximity` column improves our score. 

This was a single column. 

If we had more columns, we could improve our scores in a much more substantial way instead of throwing the information away which is what we have been doing!


---


```{python}
from sklearn import set_config
set_config(display='diagram')
main_pipe
```

Notes: 

Since there are a lot of steps happening we can use `set_config` from sklearn and it will display a diagram of what is going on in our main pipeline. 

-------


<center><img src="/module6/pipeline.png"  width = "90%" alt="404 image" /></center>


Notes: 

We can also look at this image which shows the more generic version of what happens in `ColumnTransformer` and where it stands in our main pipeline.

-------


#### Do we need to preprocess categorical values in the target column?

- Generally, there is no need for this when doing classification. 
- `sklearn` is fine with categorical labels (y-values) for classification problems. 

Notes: 

<br>

---

# Let’s apply what we learned!

Notes: <br>