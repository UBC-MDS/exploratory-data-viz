---
params:
  dynamictitle: "module5_04"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module5/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
from sklearn.compose import make_column_transformer


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module5/"
```


type: slides

# Scaling 

Notes: <br>

---

```{python include=FALSE}
bball_df = pd.read_csv('data/bball.csv')
bball_df = bball_df[(bball_df['position'] =='G') | (bball_df['position'] =='F')]
X = bball_df[['weight', 'height', 'salary']]
y =bball_df["position"]
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.20, random_state=123)
two_players = X_train.sample(2, random_state=42)
two_players
```

```{python}
two_players
```
```{python}
euclidean_distances(two_players)
```
```{python}
two_players_subset = two_players[["salary"]]
two_players_subset
```

```{python}
euclidean_distances(two_players_subset)
```


Notes: 

We left off where we saw that having the columns `weight` and `height` did not contribute to the distance between the points very much and that was because the scale of them were much less. 

So what do we do about this? 

Well, we have to scale the columns they they are all using a similar range of values! 

Luckily Sklearn has tools for this. 

---

## Scaling 

| Approach | What it does | How to update ùëã (but see below!) | sklearn implementation |
|---------|------------|-----------------------|----------------|
| Normalization | sets range to [0,1]   | `X -= np.min(X,axis=0)`<br>`X /= np.max(X,axis=0)` | [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html),
    "| Standardization | sets sample mean to 0, s.d. to 1   | `X -= np.mean(X,axis=0)`<br>`X /=  np.std(X,axis=0)` | [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) |

There are all sorts of articles on this; see, e.g. [here](http://www.dataminingblog.com/standardization-vs-normalization/) and [here](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc).


Notes: 

There are different ways to scales. 

2 popular options are called Normalization and Standardization. We are not going to explain in detail what is going on behind these tools but more so on how to implement them. 

---

## `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)

```{python}
from sklearn.preprocessing import StandardScaler
```
<br>

```{python}
scaler = StandardScaler()    # Create feature transformer object
scaler.fit(X_train) # fitting the transformer on the train split 
X_train_scaled = scaler.transform(X_train) # transforming the train split
X_test_scaled = scaler.transform(X_test) # transforming the test split
pd.DataFrame(X_train_scaled, columns = X_train.columns).head()
```


Notes: 

We'll be concentrating on `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which is a `transformer`.  

 For now, try to only focus on the syntax.
 
 We'll talk about scaling in a bit.
 
1. Create feature transformer object. this is done in a similar way to how we create a model. 
2. Fitting the transformer on the train split 
3. Transform the train split using `.transform()`
4. Then tranform the test split. 

- `sklearn` uses `fit` and `transform` paradigms for feature transformations. (In model building it was `fit` and `predict` or `score`)
- We `fit` the transformer on the train split and then `transform` the train split as well as the test split. 
- We apply the same transformations on the test split. 


---

## Scikit learn's *predict* vs *transform* 


```python
model.fit(X_train, y_train)
X_train_predictions = model.predict(X_train)
X_test_predictions = model.predict(X_test)
```


```python
transformer.fit(X_train, [y_train])
X_train_transformed = transformer.transform(X_train)
X_test_transformed = transformer.transform(X_test)
```  

```python
transformer.fit_transform(X_train)
```


Notes:

Suppose  we have a named `model` which is either a  classification or regression model. 

We can compare it with  `transformer` which is a transformer used to change the input representation like to scales numeric features.

You can pass `y_train` in `fit` but it's usually ignored. It allows you to pass it just to be consistent with usual usage of `sklearn`'s `fit` method.   

You can also carry out fitting and transforming in one call using `fit_transform`. But be mindful to use it only on the train split and **not** on the test split. 

---

```{python}
knn_unscaled = KNeighborsClassifier();
knn_unscaled.fit(X_train, y_train)
print('Train score: %0.3f' %(knn_unscaled.score(X_train, y_train)))
print('Test score: %0.3f' %(knn_unscaled.score(X_test, y_test)))
```

```{python}
knn_scaled = KNeighborsClassifier();
knn_scaled.fit(X_train_scaled, y_train)
print('Train score: %0.3f' %(knn_scaled.score(X_train_scaled, y_train)))
print('Test score: %0.3f' %(knn_scaled.score(X_test_scaled, y_test)))
```


Notes: 

Do you expect `DummyClassifier` results to change after scaling the data? 

Let's check whether scaling makes any difference for ùëò-NNs. 

The scores with scaled data are better compared to the unscaled data in case of ùëò-NNs.

We am not carrying out cross-validation here for a reason that that we'll look into soon. 

We are being a bit sloppy here by using the test set several times for teaching purposes. 

But when you build an ML pipeline, please do assessment on the test set only once.  


---

<center><img src="/module2/module2_12b.png"  width = "50%" alt="404 image" /></center>


Notes: 


---

Notes: 

There are many other hyperparameters for decision trees you can explore at the link <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank">here</a> .


---

Notes: 

<br>

---

Notes: 

---



Notes: 

---



Notes: 

---



Notes: 

---


Notes: 

---


Notes: 

---


Notes: 

---


# Let‚Äôs apply what we learned!

Notes: <br>